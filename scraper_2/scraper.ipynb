{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6c75ac0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reqeusts, bs4 import\n",
    "import requests, bs4\n",
    "# BeautifulSoup 클래스 import\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "115883fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "req_header = { \"user-agent\" : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "39491a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_occ1_id(soup):\n",
    "    occ1_id_dict = {}\n",
    "    for button in soup.select(\"ul#occ1_div li button[data-item-val]\"):\n",
    "        occ1_id = button.get(\"data-item-val\")\n",
    "        occ1_name = button.find_next(\"span\").text if button.find_next(\"span\") else \"\"\n",
    "        if occ1_id and occ1_name:\n",
    "            occ1_id_dict[occ1_id] = occ1_name\n",
    "    return occ1_id_dict\n",
    "\n",
    "\n",
    "def select_occ2_id(soup, id_list):\n",
    "    occ2_id_dict = {}\n",
    "    # 모든 occ2 관련 버튼을 한 번에 선택하고 필터링\n",
    "    all_occ2_buttons = soup.select(\"ul[id^='occ2_ul_'] li button[data-item-val]\")\n",
    "    \n",
    "    for button in all_occ2_buttons:\n",
    "        data_val = button.get(\"data-item-val\")\n",
    "        if data_val and \"_\" in data_val:\n",
    "            occ1_id, occ2_id = data_val.split(\"_\")\n",
    "            if occ1_id in id_list:\n",
    "                occ2_name = button.find_next(\"span\").text if button.find_next(\"span\") else \"\"\n",
    "                occ2_id_dict[occ2_id] = (occ1_id, occ2_name)\n",
    "    \n",
    "    return occ2_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5b956478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def calculate_page_count(count_str):\n",
    "    count = int(count_str.replace(\",\", \"\"))\n",
    "    page = ((count - 1) // 30) + 1\n",
    "    return page\n",
    "\n",
    "def get_page_url_list(occ2_id_dict):\n",
    "    count_url = \"https://job.incruit.com/s_common/searchjob/v3/searchjob_getcount_ajax.asp?occ2=\"\n",
    "    base_url = \"https://job.incruit.com/jobdb_list/searchjob.asp?articlecount=30&occ2=\"\n",
    "    \n",
    "    page_url_list = []\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(req_header)\n",
    "        \n",
    "        for occ2_id in occ2_id_dict.keys():\n",
    "            try:\n",
    "                res = session.get(count_url + occ2_id)\n",
    "                if res.ok:\n",
    "                    page_num = calculate_page_count(res.text)\n",
    "                    # 리스트 컴프리헨션 사용으로 루프 최적화\n",
    "                    new_urls = [(occ2_id, f\"{base_url}{occ2_id}&page={page_index}\") \n",
    "                               for page_index in range(1, page_num + 1)]\n",
    "                    page_url_list.extend(new_urls)\n",
    "                else:\n",
    "                    print(f\"에러 코드 = {res.status_code}, occ2_id: {occ2_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"예외 발생: {e}, occ2_id: {occ2_id}\")\n",
    "    \n",
    "    return page_url_list\n",
    "\n",
    "def first_cell_parser(li_tag):\n",
    "    first_cell = li_tag.find('div', class_='cell_first') # 기업 이름과 태그\n",
    "    company = first_cell.find('a')\n",
    "    return company.text\n",
    "\n",
    "def mid_cell_parser(li_tag):\n",
    "    mid_cell = li_tag.find('div', class_='cell_mid') # 채용 공고\n",
    "    title = mid_cell.find('a')\n",
    "    href = title[\"href\"]\n",
    "    return title.text, href\n",
    "\n",
    "def get_employ_info(id, link, session=None):\n",
    "    if session is None:\n",
    "        session = requests\n",
    "\n",
    "    employ_info = {}\n",
    "    try:\n",
    "        res = session.get(link, headers=req_header)\n",
    "        if res.ok:\n",
    "            soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "            \n",
    "            info_items = soup.select('ul.jc_list li div.txt em')\n",
    "            \n",
    "            keys = ['고용형태', '경력', '근무지역', '학력', '급여조건']\n",
    "            employ_info = {keys[i]: info_items[i].text for i in range(min(len(keys), len(info_items)))}\n",
    "        else:\n",
    "            print(f\"에러 코드 = {res.status_code}, URL: {link}\")\n",
    "    except Exception as e:\n",
    "        print(f\"정보 추출 중 오류: {e}, URL: {link}\")\n",
    "    \n",
    "    return employ_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "972cd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_jobs(page_url_list, occ1_id_dict, occ2_id_dict):\n",
    "    job_data = {occ1_name: {} for occ1_id, occ1_name in occ1_id_dict.items()}\n",
    "    \n",
    "    # HTTP 세션 재사용으로 연결 오버헤드 감소\n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(req_header)\n",
    "        \n",
    "        for id2, page_url in page_url_list:\n",
    "            try:\n",
    "                occ1_id, occ2_name = occ2_id_dict.get(id2, (None, \"알 수 없는 직업\"))\n",
    "                if occ1_id is None:\n",
    "                    continue\n",
    "                    \n",
    "                occ1_name = occ1_id_dict.get(occ1_id, \"알 수 없는 대분류\")\n",
    "                \n",
    "                # 해당 소분류 초기화 (필요시)\n",
    "                if occ2_name not in job_data[occ1_name]:\n",
    "                    job_data[occ1_name][occ2_name] = []\n",
    "                \n",
    "                res = session.get(page_url)\n",
    "                if res.ok:\n",
    "                    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "                    \n",
    "                    # 한 번에 모든 채용 정보 추출\n",
    "                    li_tag_list = soup.select(\"li.c_col\")\n",
    "                    \n",
    "                    for li_tag in li_tag_list:\n",
    "                        # 정보 추출 최적화\n",
    "                        first_cell = li_tag.find('div', class_='cell_first')\n",
    "                        company_name = first_cell.find('a').text if first_cell and first_cell.find('a') else \"\"\n",
    "                        \n",
    "                        mid_cell = li_tag.find('div', class_='cell_mid')\n",
    "                        title_tag = mid_cell.find('a') if mid_cell else None\n",
    "                        \n",
    "                        if title_tag and title_tag.get(\"href\"):\n",
    "                            title = title_tag.text\n",
    "                            title_url = title_tag[\"href\"]\n",
    "                            \n",
    "                            # 동일 세션 재사용\n",
    "                            employ_info = get_employ_info(id2, title_url, session)\n",
    "                            \n",
    "                            job_info = {\n",
    "                                \"회사명\": company_name,\n",
    "                                \"채용공고\": title,\n",
    "                                \"URL\": title_url,\n",
    "                                **employ_info\n",
    "                            }\n",
    "                            \n",
    "                            job_data[occ1_name][occ2_name].append(job_info)\n",
    "                        \n",
    "                    # 너무 짧은 sleep은 IP 차단 위험이 있으므로 주의\n",
    "                    time.sleep(0.1)  # 0.2초에서 0.1초로 단축\n",
    "                else:\n",
    "                    print(f\"페이지 접근 오류: {page_url}, 에러 코드 = {res.status_code}\")\n",
    "            except Exception as e:\n",
    "                print(f\"오류 발생: {e}, occ2_id: {id2}, URL: {page_url}\")\n",
    "    \n",
    "    return job_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b520e174",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename=\"incruit_jobs.json\"):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"{filename}에 데이터가 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "95d6cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = \"https://job.incruit.com/jobdb_list/searchjob.asp?occ1=100&occ1=101&occ1=102&occ1=150&occ1=104&occ1=160&occ1=110&occ1=106&occ1=140&occ1=120&occ1=170&occ1=103&occ1=107&occ1=190&occ1=200&occ1=210&occ1=130\"\n",
    "    \n",
    "    with requests.Session() as session:\n",
    "        session.headers.update(req_header)\n",
    "        res = session.get(url)\n",
    "\n",
    "        if res.ok:\n",
    "            html = res.text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            \n",
    "            # 대분류(occ1) 정보 가져오기\n",
    "            occ1_id_dict = select_occ1_id(soup)\n",
    "            print(f\"대분류(occ1) 개수: {len(occ1_id_dict)}\")\n",
    "            \n",
    "            # 소분류(occ2) 정보 가져오기\n",
    "            occ2_id_dict = select_occ2_id(soup, occ1_id_dict.keys())\n",
    "            print(f\"소분류(occ2) 개수: {len(occ2_id_dict)}\")\n",
    "            \n",
    "            # 각 소분류별 페이지 URL 생성\n",
    "            page_url_list = get_page_url_list(occ2_id_dict)\n",
    "            print(f\"총 페이지 수: {len(page_url_list)}\")\n",
    "            \n",
    "            # 일부만 먼저 테스트 (전체 실행 전)\n",
    "            # 전체 페이지의 10%만 먼저 테스트\n",
    "            test_size = max(1, len(page_url_list) // 100)\n",
    "            test_urls = page_url_list[:test_size]\n",
    "            \n",
    "            print(f\"테스트 샘플로 {test_size}개 페이지 처리 시작...\")\n",
    "            job_data = classify_jobs(test_urls, occ1_id_dict, occ2_id_dict)\n",
    "            \n",
    "            # 테스트 결과 확인 후 전체 실행\n",
    "            test_complete = input(\"테스트 완료. 전체 데이터를 수집하시겠습니까? (y/n): \")\n",
    "            if test_complete.lower() == 'y':\n",
    "                print(\"데이터 수집을 시작합니다...\")\n",
    "                job_data = classify_jobs(page_url_list, occ1_id_dict, occ2_id_dict)\n",
    "                save_to_json(job_data)\n",
    "            else:\n",
    "                print(\"테스트 데이터만 저장합니다.\")\n",
    "                save_to_json(job_data, \"incruit_test_jobs.json\")\n",
    "        else:\n",
    "            print(f\"초기 페이지 접근 오류. 에러 코드 = {res.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0a5f1686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "대분류(occ1) 개수: 17\n",
      "소분류(occ2) 개수: 113\n",
      "총 페이지 수: 2115\n",
      "테스트 샘플로 21개 페이지 처리 시작...\n",
      "테스트 데이터만 저장합니다.\n",
      "incruit_test_jobs.json에 데이터가 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
