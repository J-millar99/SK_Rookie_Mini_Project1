import streamlit as st
import pandas as pd
import os
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

SCRAPED_JOBS_PATH = "data/scraped_jobs.json"

# ğŸ”¸ ìŠ¤í¬ë© ì €ì¥ í•¨ìˆ˜
def save_scraped_jobs_to_file():
    with open(SCRAPED_JOBS_PATH, "w", encoding="utf-8") as f:
        json.dump(st.session_state["scraped_jobs"], f, ensure_ascii=False, indent=2)

# ğŸ”¹ ìŠ¤í¬ë© ì„¸ì…˜ ì´ˆê¸°í™”
if "scraped_jobs" not in st.session_state:
    if os.path.exists(SCRAPED_JOBS_PATH):
        with open(SCRAPED_JOBS_PATH, "r", encoding="utf-8") as f:
            st.session_state["scraped_jobs"] = json.load(f)
    else:
        st.session_state["scraped_jobs"] = []

# ğŸ”¹ ì¶”ì²œ ê²°ê³¼ ìƒíƒœ ì €ì¥ìš©
if "recommended_jobs" not in st.session_state:
    st.session_state["recommended_jobs"] = []

st.set_page_config(page_title="ë§ì¶¤ ì±„ìš© ì¶”ì²œ", layout="wide")
st.title("ğŸ¯ ë§ì¶¤ ì±„ìš© ì¶”ì²œ ì‹œìŠ¤í…œ")
st.markdown("ë‹¹ì‹ ì˜ ì •ë³´ì™€ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì–´ìš¸ë¦¬ëŠ” ì±„ìš©ê³µê³ ë¥¼ ì¶”ì²œí•´ë“œë¦½ë‹ˆë‹¤.")

# ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_data
def load_data():
    path = os.path.join("data", "cleaned_data.json")
    return pd.read_json(path)

df = load_data()

# ì‚¬ìš©ì ì…ë ¥
name = st.text_input("ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”")
interest = st.text_input("ê´€ì‹¬ ì§êµ° ë˜ëŠ” ë¶„ì•¼ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: ë°ì´í„° ë¶„ì„, ë°±ì—”ë“œ, ë§ˆì¼€íŒ…)")
spec = st.text_area("ê´€ì‹¬ ì§ë¬´ ê´€ë ¨ ê²½í—˜ì´ë‚˜ í™œë™ì„ ì…ë ¥í•´ì£¼ì„¸ìš” (ì˜ˆ: í”„ë¡œì íŠ¸, ì¸í„´ ê²½í—˜ ë“±)")

# ì¶”ì²œ ë²„íŠ¼ ëˆŒë €ì„ ë•Œë§Œ ì¶”ì²œ ê²°ê³¼ ì„¸ì…˜ì— ì €ì¥
if st.button("ğŸ” ì¶”ì²œë°›ê¸°"):
    if not interest and not spec:
        st.warning("ê´€ì‹¬ ì§êµ°ì´ë‚˜ í™œë™ ê²½í—˜ ì¤‘ í•˜ë‚˜ ì´ìƒì€ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
    else:
        user_input = interest + " " + spec
        vectorizer = TfidfVectorizer(stop_words='english')
        documents = df["ì±„ìš©ê³µê³ "] + " " + df["ì§êµ°"] + " " + df["íšŒì‚¬ëª…"]
        tfidf_matrix = vectorizer.fit_transform(documents)
        user_vector = vectorizer.transform([user_input])

        similarities = cosine_similarity(user_vector, tfidf_matrix).flatten()
        top_indices = similarities.argsort()[::-1][:5]
        st.session_state["recommended_jobs"] = df.iloc[top_indices].to_dict(orient="records")
        st.success("ì¶”ì²œ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì•„ìš”!")

# ì¶”ì²œ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ê³„ì† ë³´ì—¬ì£¼ê¸°
if st.session_state["recommended_jobs"]:
    st.subheader(f"ğŸ“‹ {name or 'ì‚¬ìš©ì'} ë‹˜ì„ ìœ„í•œ ì¶”ì²œ ì±„ìš©ê³µê³  Top 5")
    for i, job in enumerate(st.session_state["recommended_jobs"]):
        with st.expander(f"âœ… {job['ì±„ìš©ê³µê³ ']} ({job['íšŒì‚¬ëª…']})"):
            st.markdown(f"""
            - **ì§€ì—­:** {job['ì§€ì—­']}  
            - **ì§êµ°:** {job['ì§êµ°']}  
            - **ê³ ìš©í˜•íƒœ:** {job['ê³ ìš©í˜•íƒœ']}  
            - **ê²½ë ¥:** {job['ê²½ë ¥']}  
            - **[ê³µê³  ë§í¬ ë°”ë¡œê°€ê¸°]({job['URL']})**
            """)
            # â­ ìŠ¤í¬ë© ë²„íŠ¼
            if st.button("â­ ìŠ¤í¬ë©í•˜ê¸°", key=f"scrap_{i}"):
                if job not in st.session_state["scraped_jobs"]:
                    job["ê³µê³ ë§í¬"] = f"[{job['ì±„ìš©ê³µê³ ']}]({job['URL']})"  # âœ… í•„ìˆ˜!
                    st.session_state["scraped_jobs"].append(job)
                    save_scraped_jobs_to_file()
                    st.success("âœ… ì €ì¥ ì™„ë£Œ!")
                else:
                    st.warning("âš ï¸ ì´ë¯¸ ì €ì¥ëœ ê³µê³ ì…ë‹ˆë‹¤.")
